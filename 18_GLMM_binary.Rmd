---
title: "Смешанные линейные модели для бинарных данных"
subtitle: "Линейные модели..."
author: "Вадим Хайтов, Марина Варфоломеева"
output:
  beamer_presentation:
    theme: default
    toc: no
    colortheme: beaver
    latex_engine: xelatex
    slide_level: 2
    fig_crop: false
    highlight: tango
    includes:
      in_header: ./includes/header.tex
---

```{r setup, include=FALSE, cache=FALSE, purl=FALSE}
# output options
options(width = 90)
library (knitr)
library (scales)
# chunk default options
opts_chunk$set(fig.show='hold', size='footnotesize', comment='#', 
               fig.width=4.5, out.width='3in',
               fig.height=2.25, out.height='1.5in',
               cache=FALSE,
               dev='cairo_pdf',
               
               warning=FALSE, message=FALSE)
# Тема для иллюстраций
library (ggplot2)
library (gridExtra)
```



# Что мы знаем про моделирование бинарных данных {.segue}

## Бинарные данные вокруг нас


Мы встречаемся с бинарными исходами очень часто      

- Проголосовали за данного кандидата или не проголосовали 
- Прогноз сбылся или не сбылся       
- Человек нравится или не нравится      
- Состояние пациента улучшилось или ухудшилось

Бинарные данные -- очень распространенный тип зависимых переменных, они отражают соотношение положительных и отрицательных исходов в ответ на влияние предикторов. 



## Бинарные данные тоже могут быть связаны со случайными эффектами 

Бинарные данные могут находиться в связи с группирующими, случайными факторами. 

Например, нас интересует как зависит голосование за или против кандидата от пола, возраста и материального положения избирателя. 

**Задание:** Придумайте, какие могут быть группирующие (случайные) факторы в подобном исслдеовании.

\pause

В качестве группирующих факторов, не входящих в интерес исследователя, могут выступать регион, город, предприятие, к которым относятся избиратели. Все это заставляет нас рассмотреть обобщенные смешанные линейные модели (GLMM). 

Вспомним, как устроена работа с линейными моделями, описывающими поведение бинарных данных. 



## Структура данных для анализа

- При разных значениях предиктора, зависимая переменная принимает значения 1 или 0 (событие произошло или не произошло) 
- Предиктор может быть непрерывным или дискретным.
- Предикторов может быть один или много.

```{r echo=FALSE}
set.seed(123456)
x <- 1:20 

dat <- data.frame(X = rep(x, each = 3))

d <- exp(-5+ 0.5 * dat$X)
pi <- d/(1 + d) 

dat$Out <- rbinom(nrow(dat), 1, pi)
head(dat, 15)
```

## Цель анализа - логистическая кривая

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw())
ggplot(dat, aes(x = X, y = Out)) + geom_smooth(method = "glm", method.args = list(family = "binomial")) + labs(x= "Предиктор (X)", y = "Вероятность (Out = 1)")
```

\pause
- Вместо перменной отклика в виде 1 или 0 переходим к вероятностям событий ($\pi$), распределенным от 0 до 1.  
- Итогом анализа является логистическая регрессионная кривая, связывающая значения предиктора и вероятность появления события $Out_i = 1$.  


## Вероятности, шансы, логиты

- Для подбора параметров логистической кривой необходимо перейти к линейной зависимости.
\pause
- Вместо вероятностей, переходим к отношению шансов: $odds = \frac{\pi}{1-\pi}$, распределенным от 0 до $+\infty$. 
- Далее вместо отношения шансов используем логиты: $ln(odds)=ln(\frac{\pi}{1-\pi})$, которые варьируют от  $-\infty$ до $+\infty$.  

## Связь лгитов с предиктором

```{r, echo=FALSE}
M <- glm(Out ~ X, data = dat, family = "binomial")
new_data <- data.frame(X = 1:20)
pred <- predict(M, newdata = new_data, se.fit = TRUE)
new_data$fit <-pred$fit 
new_data$se <- pred$se.fit
ggplot(new_data, aes(x = X, y = fit))  + geom_ribbon(aes(ymin = fit - 2 * se, ymax = fit + 2*se), alpha = 0.3)+ geom_line(color = "blue") + labs(x= "Предиктор (X)", y = "logit")
```


- Логит-связывающая функция: 
\pause
логиты линейно связаны с предиктором: $ln\Big(\cfrac{\pi}{1 - \pi}\Big) = \beta_0 + \beta_1 X$


## Обобщенная линейная модель для бинарного отклика


$y_i \sim Binomial(n = 1, \pi_i)$

$E(y_i) = \pi_i = \cfrac{e ^ {\beta_0 + \beta_1 x_{1} + ... + \beta_{p-1}~x_{p- 1}}}{1 + e^{\beta_0 + \beta_1 x_{1} + ... + \beta_{p-1}~x_{p- 1}}}$

$ln(\cfrac{\pi_i}{1 - \pi_i}) = \eta_i$ --- функция связи логит, переводит вероятности в логиты.

$\eta_i = \beta_0 + \beta_1 x_{1\,i} + ... + \beta_{p-1\,i}~x_{p- 1\,i}$


- Для подбора параметров регрессионной модели с бинарным откликом используют обобщенные линейные модели (GLM или GLMM). 
- Подбор параметров модели основан на методе максимального правдоподобия.

```{r}
model <-  glm(Out ~ X, data = dat, family = binomial(link="logit") )
```

## Для диагностики модели с бинарным откликом необходимо проверить 

1. Независимость испытаний друг от друга (определяется дизайном сбора материала).
\pause
2. Линейность связи с предиктором (проверяется по графику рассеяния остатков).
3.  Если есть предикторы, которые не вошли в модель, то наличие паттерна в остатках по отношению к ним может говорить о нарушении линейности связи или нарушении независимости испытаний (часто выявляется при анализе данных, связанных с временными рядами). 
\pause
4. Отсутствие избыточности дисперсии (проверяется по соотношению суммы квадратов пирсоновских остатков и числа степеней свободы).

## Смысл коэффициентов модели

```{r eval=FALSE}
summary(model)
```
```
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -3.82050    1.00162  -3.814 0.000137 ***
X            0.40372    0.09666   4.177 2.96e-05 ***
```
\pause
Поскольку коэффициент $b_1$ положительный, то при увеличении предиктора на единицу отношение шансов возрастает в $e^{b_1}$ = exp(`r as.numeric((coef(model)[2]))`) = `r round(as.numeric(exp(coef(model)[2])), 1)` раз.


## Модель 

Подобранные параметры
```
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -3.82050    1.00162  -3.814 0.000137 ***
X            0.40372    0.09666   4.177 2.96e-05 ***
```

Связь, переменной отклика с предиктором описывается следующей моделью 

$E(y_i) = \pi_i = \cfrac{e ^ {-3.8 + 0.4X_i}}{1 + e ^ {-3.8 + 0.4X_i}}$

$y_i \sim Binomial(n = 1, \pi_i)$





# Пример -- морские звезды и мидии {.segue}

## Различают ли морские звезды два вида мидий: предыстория

\columnsbegin
\column{0.48\textwidth}

\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{images/Sea_star.png}

\column{0.48 \textwidth}


В Белое море, где обитают ценные промысловые моллюски, атлантические мидии, недавно вселились мидии тихоокеанские. Важно понять, могут ли хищные морские звезды регулировать численность вида-вселенца. 

Рассматривая этот пример на предыдущей лекции, посвященной бинарным данным, мы обсуждали только данные первого, разведочного эксперимента. Было показано, что шансы быть съеденными у вида-вселенца выше, чем у коренного вида. 


\tiny{Данные: Khaitov et al, 2018}

\columnsend



## Случайные факторы, как инструмент поверки результатов на прочность

Один из способов добавить вес полученным выводам - это устроить проверку гипотезы на материале, включающем случайные факторы:


- Многократно повторить однотипный эксперимент.
- Желательно, чтобы эксперименты проводили несколько разных команд экспериментаторов.
- В каждом эксперименте установить несколько независимых модулей, в каждом из которых идет изучаемый процесс.

Если и в таком разнородном материале будет выявляться поведение системы, соответствующее вашей гипотезе, то значит это **закономерность**, а не игра случая. 


## Случайный фактор первого уровня

\columnsbegin
\column{0.48\textwidth}

\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{images/Containers.jpg}


\column{0.48 \textwidth}

Предыстория: разведочный эксперимент проводили в четырех независимых контейнерах. Мы включили фактор `Box` в фиксированную часть модели. Однако влияние этого фактора вне интереса исследования. 

По своей природе фактор `Box` --- случайный. Для надежности оценки случайного эффекта лучше использовать большое количество градаций этого фактора. 

Для проведения нового исследования мы использовали от 9 до 12 экспериментальных контейнеров. 

\columnsend

## Полезно сомневаться: cлучайный фактор второго уровня

А не являются ли результаты одного конкретного эксперимента (даже если он однотипно воспроизводится в разных контейнерах) просто результатом счастливого стечения обстоятельств? Например, погода в период проведения одного эксперимента была какая-то особенная.

Важно повторить эксперимент несколько раз. В новом исследовании мы провели 4 эксперимента (по два 2015 и 2016 гг.).

Фактор "Эксперимент" вне интереса исследования, по своей природе --- это тоже случайный фактор, но более высокого уровня, чем фактор `Box`. 

## Иерархия факторов

\includegraphics[width=\textwidth,height=0.7\textheight,keepaspectratio]{images/Factors_hierarchy.png}

Случайный фактор `Box` иерархически подчинен (nested in) фактору `Experiment`. 


## Фактор "Год"
\pause
Эксперимент проводили в два разных года. Для проверки нашей гипотезы важно понять устойчив ли эффект, который наблюдали в разведочном эксперименте, от года к году. 
Фактор `Year` надо включить в анализ, но в качестве фиксированного фактора.



## Читаем данные

```{r}
astr2 <- read.csv('data/aster_mussel_full.csv', header = TRUE)
head(astr2)
```

## Наводим порядок в кодировке переменных

Год закодирован числами, его надо сделать фактором

```{r}
astr2$Year <- factor(astr2$Year)
```

Остальные категориальные переменные тоже должны быть факторами

```{r}
astr2$Box <- factor(astr2$Box)
astr2$Sp <- factor(astr2$Sp)
```

Переменная отклика должна кодироваться как 1 (`eaten`) или 0 (`not_eaten`).


```{r}
astr2$Out <- ifelse(test = astr2$Outcome == 'eaten', yes = 1,  no = 0)
```


## Знакомимся с данными

Нет ли пропущенных значений?

```{r}
colSums(is.na(astr2))
```

## Каковы объемы выборок?

```{r}
table(astr2$Box)
```

## Нет ли коллинеарности 
<!-- \columnsbegin -->
<!-- \column{0.6\textwidth} -->

```{r}
library(cowplot); library(ggplot2); theme_set(theme_bw())

Pl_Sp <- ggplot(astr2, aes(x = Sp, y = L)) + geom_boxplot()
Pl_exp <- ggplot(astr2, aes(x = Experiment, y = L)) + geom_boxplot()
Pl_year <- ggplot(astr2, aes(x = Year, y = L)) + geom_boxplot()
plot_grid(Pl_Sp, Pl_exp, Pl_year, ncol = 3)
```
<!-- \column{0.38\textwidth} -->

<!-- \vspace{7\baselineskip} -->

В чем проблема?
\pause
Размер распределен более-менее равномерно между градациями, коллинеарности нет. 
Однако в 2016 году была проведена более жесткая сортировка по размеру.


<!-- \columnsend -->

## Есть ли выбросы?

<!-- \columnsbegin -->
<!-- \column{0.6\textwidth} -->

```{r, fig.height=1.88, out.height='1.25in'}
ggplot(astr2, aes(y = 1:nrow(astr2))) + geom_point(aes(x = L) )
```

<!-- \column{0.38\textwidth} -->

\vspace{3\baselineskip}

Выбросов нет, но проблема разного разброса в разные годы опять видна. В модель обязательно надо включить взаимодействие `Year:L`

<!-- \columnsend -->




# Подбираем модель {.segue}

## Компоненты модели, котоую мы строим 

\columnsbegin
\column{0.48\textwidth}

**Фиксированная часть модели**    
Предикторы:    
- `Sp` - дискретный фактор (градации: `Tr`, `Ed`)             
- `Year` - дискретный фактор (градации: `2015`, `2016`)          
- `L` - непрерывный предиктор          
- Взаимодействия первого порядка `Sp:Year`, `Sp:L`, `L:Year`         
- Взаимодействия второго порядка `Sp:Year:L`         

\column{0.48\textwidth}

**Случайная часть модели**     
- Эффект эксперимента            
- Эффект контейнера в пределах эксперимента          
- Остатки     

\columnsend


## Модель, которую мы строим

$Out_i \sim Binomial(n = 1, \pi_i)$

$E(y_i) = \pi_i$

$ln(\cfrac{\pi_i}{1 - \pi_i}) = \eta_i$ --- функция связи логит, переводит вероятности в логиты.

$$\pmb{\eta}_{ijk} = \mathbf X \pmb{\beta} + \mathbf S \mathbf d + \mathbf Z \mathbf b $$ 


где $i$ -- наблюдение, $jk$ -- j-й контейнер в k-м эксперименте, $k$ -- эксперимент.

$\mathbf X$ - модельная матрица, описывающая фиксированные предикторы.

$\mathbf S$ - матрица, описывающая распределение испытаний по контейнерам.

$\mathbf Z$ - матрица, описывающая распределение испытаний по экспериментам.

$\mathbf d_{jk}$ - дисперсия, связанная с контейнерами в пределах эксперимента. 

$\mathbf b_k$ - дисперсия, связанная с экспериментами. 



## Модель со случайным свободным членом (random intercept model)

```{r, warning=TRUE}
library(lme4)
model1_ri <- glmer(Out ~ L*Sp*Year + 
                  (1|Experiment/Box) , data = astr2, 
                family = binomial(link = "logit"))

```
Мы получили предупреждение (`Warning`), что модель не сошлась. 

## Если модель не сходится

Это обычная история для функции `glmer()`, в которой реализуются очень сложные числовые итерационные решения. 

Также справку по этой проблеме можно посмотреть, набрав 

```{r eval=FALSE}
?convergence
```

## Если модель не сходится

\small
Для исправления можно пойти по нескольким путями 

1. В первую очередь надо проверить нет ли каких-то проблем с данными (ошибки в набивке)     
2. Часто проблему можно решить за счет стандартизации непрерывных предикторов.      

3. Можно настроить вычислительный алгоритм функции `glmer()`. Потребуется совет  программистов и не всегда сработает (универсального решения нет). Про настройку алгоритма можно почитать на сайте https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html или на  форуме https://stackoverflow.com/       

4. Можно подобрать модель с помощью другого пакета (помимо `lme4`, GLMM реализованы еще в пакетах `glmmADMB`, `MASS`, `glmmML`), но там могут всплыть свои проблемы и появиться новые ограничения.        

5. Можно упростить модель (не рассматривать, если возможно, взаимодействия предикторов, или убрать из модели взаимодействия высоких порядков).     



## Cтандартизация непрерывного предиктора

```{r, warning=TRUE}
astr2$L_scaled <- as.numeric(scale(astr2$L))

model1_ri <- glmer(Out ~ L_scaled*Sp*Year + 
                  (1|Experiment/Box) , data = astr2, 
                family = binomial(link = "logit"))
```

Все посчиталось.

## Модель со случайным свобобным членом и случайным угловым коэффициентом (random intercept and random slope model)

Характер связи зависимой переменной с предикторами может меняться от контейнера к контейнеру и от эксперимента к эксперименту. Мы должны проверить не улучшит ли это модель.

```{r, warning=TRUE}
model1_rsi_1<- glmer(Out ~ L_scaled * Sp * Year + (1 + Sp|Experiment/Box) , 
               data = astr2, family = binomial(link = "logit"))

model1_rsi_2 <- glmer(Out ~ L_scaled * Sp * Year + (1 + L_scaled |Experiment/Box) , 
            data = astr2, family = binomial(link = "logit"))
```

Обе модели не сошлись. 




## Настраиваем алгоритм функции `glmer()`

В данном случае мы увеличили количество итераций. 

```{r, warning=TRUE}
model1_rsi_1<- glmer(Out ~ L_scaled * Sp * Year + (1 + Sp|Experiment/Box) , 
               data = astr2, family = binomial(link = "logit"),
               control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

model1_rsi_2 <- glmer(Out ~ L_scaled * Sp * Year + (1 + L_scaled |Experiment/Box) , 
            data = astr2, family = binomial(link = "logit"),
            control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))
```

Сработало. Но не факт, что сработает в других моделях.



## Сравниваем три модели

\pause


```{r}
AIC(model1_ri, model1_rsi_1, model1_rsi_2)
```

Останавливаем выбор на модели со случайным отрезком `model1_ri`






## Диагностика модели: линейность связи

```{r}
library(ggplot2)
model1_diagn <- fortify.merMod(model1_ri)
ggplot(model1_diagn, aes(x = .fitted, y = .scresid)) + geom_point() + 
  geom_smooth() 

```

Нарушений линейности нет


## Диагностика модели: избыточность дисперсии



```{r}
overdisp_fun <- function(model) {
    rdf <- df.residual(model)
    rp <- residuals(model,type="pearson")
    Pearson.chisq <- sum(rp^2)
    prat <- Pearson.chisq/rdf
    pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)
    c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)
}

overdisp_fun(model1_ri)
```

Избыточной дисперсии нет


# Дорабатываем модель  {.segue}


## Задание: Проведите упрощение модели в соответствии с протоколом backward selection

\pause

```{r}
drop1(model1_ri)
```

```{r}
model2 <- update(model1_ri, .~.-L_scaled:Sp:Year)
```

## Упроаем модель: шаг 2.

```{r}
drop1(model2)
model3 <- update(model2, . ~ . - L_scaled:Year)
```

## Упроаем модель: шаг 3.

```{r}
drop1(model3)
model4 <- update(model3, . ~ . - L_scaled:Sp)
```

## Упрощаем модель: шаг 4.

```{r}
drop1(model4)
model5 <- update(model4, . ~ . - Sp:Year)
```


## Упрощаем модель: шаг 5.

```{r}
drop1(model5)
model6 <- update(model5, . ~ . - Year)
```

## Финальная модель
```{r}
drop1(model6)
```
Больше ничего удалить нельзя. 

`model6` -- финальная модель

## Начальная и финальная модель

```{r}
AIC(model1_ri, model6)
```

Упрощение модели не привело к ее ухудшению. Мы удалили из фиксированной части модели избыточные предикторы и взаимодействия. 

В фиксированной части модели осталось только два предиктора 

```{r}
summary(model6)$call
```




## Диагностика финальной модели: линейность связи

```{r}
model6_diagn <- fortify.merMod(model6)
ggplot(model6_diagn, aes(x = .fitted, y = .scresid)) + geom_point() + 
  geom_smooth() 

```

Нарушений линейности нет


## Диагностика финальной модели: избыточность дисперсии

```{r}
overdisp_fun(model6)
```

Избыточной дисперсии нет



## Первые итоги подбора модели

В фиксированной части финальной модели осталось только два пердиктора (`L_scaled` и `Sp`), не взаимодействующих друг с другом. То есть выбор звездами того или иного вида жертв не зависит от их размера.

**Важно:** в финальную модель не вошел фактор `Year`. Это означает, что в разные годы характер связи переменной отклика с предикторами оставался одним и тем же.

## Смотрим в `summary()` 

Что вы можете сказать о случайных и фиксированных эффектах?

\footnotesize
```{r}
summary(model6)
```

## Какова роль фиксированных предикторов?

Мы получили результаты, говорящие о значимости связи отклика с фиксированными предикторами

```
Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.87617    0.18673 -10.047  < 2e-16 ***
L_scaled    -0.44467    0.08155  -5.453 4.96e-08 ***
SpTr         1.05869    0.15626   6.775 1.24e-11 ***
```

Дайте трактовку полученным коэффициентам
\pause

Отношения шансов быть съеденной изменяются в $e^{-0.44467}$ = `r round(exp(fixef(model6)[2]), 1)` раз при изменении стандартизированной длины на единицу.

Отношения шансов быть съеденной у мидии из группы `Tr` (вид-вселенец) выше, чем у мидии из группы `Ed` (коренной вид),  в $e^{1.05869}$ = `r round(exp(fixef(model6)[3]), 1)` раза. 


## Проблема ICC для GLMM?

Внутриклассовая корреляция (intraclass correlation coefficient, ICC) очень удобный показатель для оценки роли случайных эффектов. 

Чем ниже ICC, тем ниже роль группирующих факторов

ICC можно интерпретировать, как долю изменчивости объясненной наличием группирующего фактора. 

В случае GLMM (при биномиальном или пуассоновском распределении отклика) величина дисперсии закономерно связана с матожиданием, следовательно она изменяется вместе с изменением предикторов, входящих в фиксированную часть модели. Это не позволяет вычислить  напрямую, как мы это делали в случае LMM.

## Приблизительные значения ICC 

Для биномиальных GLMM приблизительную оценку ICC можно получить с помощью функции `icc()` из пакета `performance`


```{r}
library(performance)
icc(model6)
```

Все ICC очень небольшие, следовательно роль пространственных различий (разные контейнеры в пределах эксперимента) и временных различий (разные эксперименты) невелика.

Следовательно мы воспроизводили более или менее стандартные условия, которые не менялись при повторении экспериментов.


<!-- ## Какова роль случайных факторов? -->

<!-- Роль случайных факторов можно оценить по величине $pseudo R^2$. -->

<!-- ```{r} -->
<!-- library(MuMIn) -->
<!-- r.squaredGLMM(model6) -->
<!-- ``` -->


# Визуализация модели {.segue}


## Два способа визуализаци
Описание роли случайных эффектов не является задачей исследования. Нас интересует  характер связи отклика с фиксированными предикторами. 

В фокусе исследования был вопрос о различиях в вероятности быть съеденной между двумя видами мидий, однако в финальной модели осталась и непрерывная ковариата.  

Визуализировать модель можно двумя способами: в виде логистических кривых и в виде столбчатой диаграммы.

## Подготовка к визуализации в виде логистических кривых 


```{r}
logit_back <- function(x) exp(x)/(1 + exp(x)) # обратная логит-трансформация

library(dplyr)
new_data <- astr2 %>% group_by(Sp) %>% 
  do(data.frame(L_scaled = seq(min(.$L_scaled),max(.$L_scaled),
                               length.out = 100)))

X <- model.matrix(~  L_scaled + Sp, data = new_data)
b <- fixef(model6)

new_data$fit_eta <- X %*% b 
new_data$se_eta <- sqrt(diag(X %*% vcov(model6) %*% t(X)))

new_data$fit_pi <- logit_back(new_data$fit_eta)
new_data$lwr <- logit_back(new_data$fit_eta - 2 * new_data$se_eta)
new_data$upr <- logit_back(new_data$fit_eta + 2 * new_data$se_eta)

```

## Логистические кривые

```{r}
Pl_log <- ggplot(new_data, aes(x = L_scaled, y = fit_pi)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = Sp), alpha = 0.2) + 
    geom_line(aes(color = Sp)) + 
  labs(x = "Стандартизированная длина", y = "Вероятность \n быть съеденной" )
Pl_log
```

Согласно модели, чем больше длина мидии, тем меньше вероятность быть съеденной. 
Вероятность быть съеденной у мидий из группы `Tr` выше, чем у мидий из группы `Ed`. 





## Задание: Визуализируйте модель в виде столбчатой диаграммы

В фокусе исследования было сравнение вероятности быть съеденной у мидий двух групп: `Tr` vs `Ed`, а `L_scaled` - лишь ковариата, влияние которой нас может не интересовать.  

Эффект влияния дискретного предиктора  лучше отразить в виде столбчатой диаграммы, отражающей предсказание модели при среднем значении ковариаты. 

\pause

Ковариата стандартизована, ее среднее равно нулю.

```{r}
new_data <- data.frame(Sp = c("Tr", "Ed"), L_scaled = 0)

X <- model.matrix(~  L_scaled + Sp, data = new_data)
b <- fixef(model6)
new_data$fit_eta <- X %*% b 
new_data$se_eta <- sqrt(diag(X %*% vcov(model6) %*% t(X)))
new_data$fit_pi <- logit_back(new_data$fit_eta)
new_data$lwr <- logit_back(new_data$fit_eta - 2 * new_data$se_eta)
new_data$upr <- logit_back(new_data$fit_eta + 2 * new_data$se_eta)
```

## Столбчатая диаграмма

```{r}
ggplot(new_data, aes(x = Sp, y = fit_pi)) + 
  geom_col(fill = "gray") + 
  geom_errorbar(aes(ymin = lwr, ymax = upr), width = 0.2) + 
  labs(x = "Вид мидий", y = "Вероятность \n быть съеденной" )
```


# Дополнительные штрихи к модели {.segue}


## Проблема представления первичных данных при визуализации модели

На графике, визуализирующем модель, полезно приводить первичные данные. Если модель хороша, то первичные данные группируются вокруг соответствующей линии регрессии. 

Однако переменная отклик в нашем случае принимает значение 1 или 0. Такие данные трудно визуализировать напрямую. 

Можно отразить долю съеденных моллюсков каждого вида среди особей разных размерных классов. То есть отразить поведение некоторых усредненных данных. Это не первичные данные в прямом смысле. 


## Разбиваем на размерные классы приблизительно рвного объема 

```{r}
astr2$Size_class <- ntile(astr2$L_scaled, 10)
table(astr2$Size_class, astr2$Sp)
```

## Средние показатели в каждом из размерных классов 

```{r}
Mean_Out <- astr2 %>% group_by(Size_class, Sp) %>% 
  do(data.frame(Out = mean(.$Out), L_scaled = mean(.$L_scaled)))
Pl_log + geom_point(data = Mean_Out, aes(x = L_scaled, y = Out, color = Sp))
```

Точки отражают долю съеденных моллюсков в пределах каждого вида среди особей одинаковых размеров (без учета контейнера и эксперимента)


## Еще один штрих: тестовая выборка

Гипотезу о том, что хищники с большей вероятностью атакуют мидий-вселенцев, мы подвергли суровому испытанию, проведя многочисленные дополнительные исследования.

Дополнительной проверкой будет работоспособность модели на данных, не включенных в анализ.

## Тестовая выборка

В идеале тестовая выборка должна быть собрана  специально по той же методике, что и выборка, лежащая в основе модели. Или тестовую выборку можно получить путем случайного разделения данных на обучающую и тестирующую части (кросс-валидация).

Если модель работает, то она должна давать предсказания близкие к наблюдаемым. Посмотрим, произойдет ли это если мы используем данные пилотного эксперимента, которые мы не использовали при подборе модели.

**Важно** Это учебный пример. В реальных исследованиях лучше не применять данные, которые уже были использованы для формулировок каких-то гипотез, в повторном анализе.


```{r}
astr_test <- read.csv('data/aster_mussel.csv', header = TRUE)
```


## Переподберем модель для нестандартизированной ковариаты

В тестовой выборке мидии имеют другие размеры, нежели в наборе данных, на которых была построена модель. Стандартизованная ковариата не годится. 

```{r, warning=TRUE}
model6_unscaled <- glmer(Out ~ L + Sp + 
                    (1|Experiment/Box) , data = astr2, 
                    family = binomial(link = "logit"))
```



## Ничего не изменилось по сути  

```{r, eval=FALSE}
summary(model6)
```


```
Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.87617    0.18673 -10.047  < 2e-16 ***
L_scaled    -0.44467    0.08155  -5.453 4.96e-08 ***
SpTr         1.05869    0.15626   6.775 1.24e-11 ***
```

```{r, eval=FALSE}
summary(model6_unscaled)
```

```
Fixed effects:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.01862    0.53776   1.894   0.0582 .  
L           -0.11740    0.02153  -5.453 4.96e-08 ***
SpTr         1.05871    0.15626   6.775 1.24e-11 ***
```


## Предсказания для новых данных

Задание: Сделайте предсказания для новых данных

\pause

```{r}
X <- model.matrix(~ L + Sp, data = astr_test)
b <- fixef(model6_unscaled)
astr_test$predicted_pi <- logit_back(X %*% b)
```

## Визуализация связи наблюдений и предсказаний.

Задание: Предложите способ визуализировать соотношение предсказанных и наблюдаемых значений.
\pause

```{r}
ggplot(astr_test, aes(x = Outcome, y = predicted_pi)) + 
  geom_boxplot() + labs(x = "Наблюдаемый исход", 
                        y = "Предсказанная вероятность \n быть съеденной")
```

Для реально съеденных мидий предсказанная вероятность быть съеденной, в среднем, выше, чем для особей, на которых звезды не напали.  


# Summary: Что мы знаем  {.segue}

## Общие черты GLM и GLMM для бинарных откликов

В целом идеи лежащие в основе анализа аналогичны идеям GLM для бинарных откликов:

- В основе анализа стоит подбор параметров логистической регрессионной модели.   
- Параметры логистической регрессии подбираются методом максимального правдоподобия.
- Угловые коэффициенты логистической регрессии позволяют сказать во сколько раз изменяется соотношение шансов для события при увеличении предиктора на единицу (или при переходе от базового уровня фактора к данному уровню).   
- Для визуализации результатов лучше проводить обратное логит-преобразование и отражать зависимую переменную в терминах вероятностей.


## Оcобенности GLMM для бинарных откликов

- Роль случайных факторов в GLMM в основе такая же, как и в случае с LMM. Группирующие факторы могут определять дисперсию свободного члена модели или дисперсию углового коэффициента и свободного члена модели.     
- GLMM требует очень больших вычислительных ресурсов и далеко не всегда параметры модели легко вычисляются, часто модели не сходятся.
- Для лучшего схождения моделей их не надо чрезмерно усложнять.
- Часто помогает стандартизация непрерывных предикторов. 
- Внутриклассовые корреляции могут быть вычислены лишь приблизительно, но их рассмотрение дает важную информацию. Если ICC равен нулю, то лучше отказаться от рассмотрения случайных факторов и применить обычную GLM. 



## Что почитать
+ Zuur, A.F. et al. 2009. Mixed effects models and extensions in ecology with R. - Statistics for biology and health. Springer, New York, NY. 

