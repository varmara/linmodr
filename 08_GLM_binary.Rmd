---
title: "Регрессионный анализ для бинарных данных"
subtitle    : "Линейные модели..."
author: Вадим Хайтов, Марина Варфоломеева
output:
  ioslides_presentation:
    widescreen: yes
    css: assets/my_styles.css
    logo: assets/Linmod_logo.png
editor_options: 
  chunk_output_type: console
---


```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE, cache = FALSE)
```


## Мы рассмотрим 
+ Регрессионный анализ для бинарных зависимых переменных

### Вы сможете
+ Построить логистическую регрессионную модель, подобранную методом максимального правдоподобия
+ Дать трактовку параметрам логистической регрессионной модели 
+ Провести анализ девиансы, основанный на логистической регрессии


## Бинарные данные - очень распространенный тип зависимых переменных

+ Вид есть - вида нет
+ Кто-то в результате эксперимента выжил или умер
+ Пойманное животное заражено паразитами или здорово
+ Команда выиграла или проиграла 

и т.д.

## На каком острове лучше искать ящериц? {.columns-2}

<img src="images/esher.jpg" width="500" height="500" >  

Пример взят из книги Quinn & Keugh (2002), Оригинальная работа Polis et al. (1998)   
```{r, echo=FALSE}
liz <- read.csv("data/polis.csv")
head(liz)
```

## Зависит ли встречаемость ящериц от размера острова? 
<div class = 'columns-2'>

<small>
Обычную линейную регрессию подобрать можно,  

*Зависимая переменная*: PA - (есть ящерицы "1" - нет ящериц "0")   
*Предиктор*  - PARATIO (отношение периметра к площади)


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4, fig.height=4.2, fig.align='left'}
library(ggplot2)

ggplot(liz, aes(x=PARATIO, y=PA)) + geom_point() + geom_smooth(method="lm", se=FALSE)
```

```{r, echo=FALSE}
fit <- lm(PA ~ PARATIO, data = liz)
summary(fit)
```

</small>

**но она категорически не годится**


</div>


## Эти данные лучше описывает логистическая кривая {.columns-2}

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=5}

ggplot(liz, aes(x=PARATIO, y=PA)) + geom_smooth(method="glm", method.args = list(family="binomial"), se=FALSE, size = 2) + ylab("Предсказанная вероятность встречи") + geom_point()
```

Логистическая кривая описывается такой формулой

$$ \pi(x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}} $$


## Зависимую величину можно преобразовать в более удобную для моделирования форму 

> 1. Дискретный результат: 1 или 0
> 2. Дискретные данные можно преобразовать в форму оценки вероятности события: $\pi = \frac{N_i}{N_{total}}$, непрерывная величина, варьирующая от 0 до 1
> 3. Вероятность события можно выразить в форме шансов (odds): $odds=\frac{\pi}{1-\pi}$ варьируют от 0 до $+\infty$. *NB: Если шансы > 1, то вероятность события, что $y_i=1$ выше, чем вероятность события $y_i = 0$.  Если шансы < 1, то наоборот*. В обыденной речи мы часто использем фразы, наподобие такой "*шансы на победу 1 к 3*"
> 4. Шансы преобразуются в _Логиты_ (logit):  $ln(odds)=\ln(\frac{\pi}{1-\pi})$ варьируют от  $-\infty$ до $+\infty$. Логиты гораздо удобнее для построения моделей.


## Логистическая модель после логит-преобразования становится привычной линейной моделью!
###Немного алгебры
Обозначим для краткости $\beta_0 + \beta_1x \equiv z$  

Тогда, логистическая модель примет такую форму 
$$ g(x)=\ln(\frac{\pi(x)}{1-\pi(x)})= \ln(\frac{\frac{e^z}{1+e^z}}{1-\frac{e^z}{1+e^z}}) $$

##Немного алгебры

Из курса алгебры мы помним, что логарифм отношения равен разности логарифмов
$$ g(x)=\ln(\frac{e^z}{1+e^z}) - \ln({1-\frac{e^z}{1+e^z}}) $$

Тогда...     

>- $$ g(x)=\ln(\frac{e^z}{1+e^z}) - \ln({\frac{1+e^z - e^z}{1+e^z}}) = \ln(\frac{e^z}{1+e^z}) - \ln({\frac{1}{1+e^z}})  $$ 
>- $$ g(x)=\ln(e^z) - \ln(1+e^z) - (\ln(1) -\ln(1+e^z))  $$ 
>- $$ g(x)=\ln(e^z) - \ln(1+e^z) - 0 +\ln(1+e^z) = \ln(e^z) = z $$ 



## Логистическая модель после логит-преобразования становится линейной

$$ g(x)=\ln(\frac{\pi(x)}{1-\pi(x)})=\beta_0 + \beta_1x$$

Остается только подобрать параметры этой линейной модели: $\beta_0$ (интерсепт) и $\beta_1$ (угловой коэффициент)    

## Метод максимального правдоподобия

###Вспомним  
Если остатки не подчиняется нормальному распределению, то метод наименьших квадратов не работает.   
В этом случае применяют _Метод максимального правдоподбия_

В результате итеративных процедур происходит подбор таких значений коэффициентов, при которых правдоподобие - вероятность получения имеющегося у нас набора данных - оказывается максимальным, при условии справедливости данной модели.

$$ Lik(x_1, ..., x_n) = \Pi^n _{i = 1}f(x_i; \theta)$$

где $f(x; \theta)$ - функция плотности вероятности с параметрами $\theta$


# Кратко о методе макcимального правдоподобия

##Правдоподобие для нормального распределения {.columns-2}

```{r gg-norm-tunnel, echo=FALSE, fig.height=6, fig.align='left', purl=FALSE}
## Based on code by Arthur Charpentier:
## http://freakonometrics.hypotheses.org/9593
## TODO: wrap it into a function and adapt it for use with other distributions
## as Markus Gesmann has done here
## http://www.magesblog.com/2015/08/visualising-theoretical-distributions.html

n <- 2


xy <- data.frame(X = rep(1:10, 3))

xy$Y <- 10*xy$X + rnorm(30, 0, 15)

xy$predicted <- predict(lm(Y ~ X, data = xy))


# X <- sqrt(brain$MRINACount) 
# Y <- sqrt(brain$PIQ)

X <- xy$X
Y <- (xy$Y)


df <- data.frame(X , Y)

# regression
reggig <- glm(Y ~ X, data = df, family = gaussian(link = "identity"))

# empty plot
vX <- seq(min(X) - 0.1, max(X) + 0.1, length = n)
vY <- seq(min(Y) - 10, max(Y) + 10, length = n)
mat <- persp(x = vX, y = vY, z = matrix(0, n, n), 
             zlim = c(0, 0.1),
             theta =  - 45, phi = 20, expand = 0.1,
             ticktype  = "detailed",  box = FALSE, border = "gray60")

x <- seq(min(X), max(X), length = 501)

# expected values
C <- trans3d(x, predict(reggig, newdata = data.frame(X = x), type = "response"), rep(0, length(x)), mat)
lines(C, lwd = 2)

sdgig <- sqrt(summary(reggig)$dispersion)

# 1SD
y1 <- qnorm(.95, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y1, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")
y2 <- qnorm(.05, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y2, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")

# C <- trans3d(c(x, rev(x)), c(y1, rev(y2)), rep(0, 2 * length(x)), mat)
# polygon(C, border = NA, col = "yellow")

# data points
C <- trans3d(X, Y, rep(0, length(X)), mat)
points(C,  pch = 16, col = "red", cex = 0.6)

# density curves
n <- 10
vX <- seq(min(X), max(X), length = n)

mgig <- predict(reggig, newdata = data.frame(X = vX))

sdgig <- sqrt(summary(reggig)$dispersion)

for(j in n:1){
  stp <- 251
  x <- rep(vX[j], stp)
  y <- seq(min(min(Y) - 10, 
               qnorm(.05, 
                     predict(reggig, 
                             newdata = data.frame(X = vX[j]), 
                             type = "response"),  
                     sdgig)), 
           max(Y) + 10, 
           length = stp)
  z0 <- rep(0, stp)
  z <- dnorm(y,  mgig[j],  sdgig)
  C <- trans3d(c(x, x), c(y, rev(y)), c(z, z0), mat)
  polygon(C, border = NA, col = "light blue", density = 40)
  C <- trans3d(x, y, z0, mat)
  lines(C, lty = 2, col = "grey60")
  C <- trans3d(x, y, z, mat)
  lines(C, col = "steelblue")
}

```

Симулированный пример, в котором 

$$
y_i = 10x_i + \varepsilon_i \\
\varepsilon \in N(0, 15)
$$

Точки (данные) - это выборки из нескольких "локальных" генеральных совокупностей с нормальным распределением зависимой переменной (каждая совокупность соответствует одному значению предиктора).    

Линия регрессии проходит через средние значения нормальных распределений.   

Параметры прямой подбираются так, чтобы вероятность существования данных при таких параметрах была бы максимальной.   


## Правдоподобие для нормального распределения 
Аналогичная картинка, но с использованием `geom_violin()`
```{r, echo=FALSE, fig.height=5.5}

# xy <- data.frame(X = rep(1:10, 3))
# xy$Y <- 10*xy$X + rnorm(30, 0, 10)
# xy$predicted <- predict(lm(Y ~ X, data = xy))
# 

rand_df <- matrix(rep(NA,100000), ncol = 10)
for(i in 1:10) rand_df[,i] <- rnorm(10000, xy$predicted[i], 10)

rand_df <- data.frame(X = rep(xy$X, each = 10000), Y = as.vector(rand_df))



ggplot(xy, aes(x = X, y = Y)) + geom_violin(data = rand_df, aes(x = factor(X)), adjust = 5 ) + geom_point() + geom_smooth(method = "lm", se = F) + geom_point(data = xy, aes(x = X, y = predicted), color = "red", size = 3) + labs(x = "Предиктор", y = "Зависимая переменная") 
```
 
 
##Нормальное распределение
То, что мы в быту привыкли называть **распределением** - это функция плотности вероятности.

**Плотность вероятности** - это функция, описывающая вероятность получения разных значений случайной величины




## Нормальное распределение {.columns-2}


```{r, echo=FALSE, fig.width=5, fig.height=6, warning=FALSE}
library(ggplot2)
mu1 <- 10
mu2 <- 20
sigma1 <- 5
sigma2 <- 10
y <- -20:50
pi <- data.frame(y = rep(y, 4), pi = c(dnorm(y, mu1, sigma1), dnorm(y, mu1, sigma2), dnorm(y, mu2, sigma1), dnorm(y, mu2, sigma2)), mu = rep(c(mu1, mu2), each = 2*length(y)), sigma = rep(c(sigma1, sigma2, sigma1, sigma2), each = length(y)) )

ggplot(pi, aes(x = y, y = pi)) + geom_line(stat = "identity") + facet_grid(mu~sigma, labeller = label_both, scales = "free_y") + ggtitle("Нормальное распределение \nпри разных параметрах") + ylab("Плотность вероятности (f)")

```

$$f(y;\mu, \sigma)= \frac {1}{\sigma \sqrt{2 \pi}} e^{-\frac{(y-\mu)^2}{2\sigma^2}}$$

### Два параметра ($\mu$, $\sigma$)

Среднее: &emsp; $E(Y)  = \mu$

Дисперсия: $var(Y) = \sigma^2$

### Пределы варьирования   

$-\infty \le Y \le +\infty$    


## Биномиальное распределение

$f(y; N, \pi) = \frac{N!}{y! \times (N-y)!} \times \pi^y \times (1 - \pi)^{N-y}$

<div class="columns-2">

```{r, echo=FALSE, fig.width=5, fig.height=5}
mu1 <- 0.1
mu2 <- 0.5
N1 <- 10
N2 <- 30

y <-seq(0, 30, 1)

pi <- data.frame(y = rep(y, 4), pi = c(dbinom(y, size = N1, prob = mu1), dbinom(y,  size = N2, prob = mu1), dbinom(y,  size = N1, prob = mu2), dbinom(y,  size = N2, prob = mu2)),  mu = rep(c(mu1, mu2), each = 2*length(y)), N = rep(c(N1, N2, N1, N2), each = length(y)))

ggplot(pi, aes(x = y, y = pi)) + geom_bar(stat = "identity") + facet_grid(N~mu,  scales = "free_y", labeller = label_both) + ggtitle("Биномиальное распрделение \n при разных параметрах") + ylab("Плотность вероятности (f)")

```

<small>

### Два параметра ($N$, $\pi$)

Среднее: &emsp;&emsp; $E(Y)  = N \times \pi$  
Дисперсия: $var(Y) = N \times \pi \times (1-\pi)$  
Параметр $N$ определяет количество объектов в испытании  
Параметр $\pi$ - вероятность события ($y = 1$)

### Пределы варьирования

$0 \le Y \le +\infty$, &emsp; $Y$ **целочисленные**
</small>


</div>


## Правдоподобие для биномиального распределения

```{r, echo=FALSE, fig.height=5, fig.width=8, warning=FALSE, message=FALSE}

xy <- data.frame(X = rep(seq(1,50, 5), 3))
p <- -0.2*xy$X + 5
p <- exp(p) / (1 + exp(p))

Size = length(xy$X)

xy$Y <- rbinom(30, Size, p)/Size

Mod <- glm(Y ~ X, data = xy, family = "binomial")



xy$predicted <- predict(Mod, type = "response")


rand_df <- matrix(rep(NA,100000), ncol = 10)



for(i in 1:10) rand_df[,i] <- rbinom(10000, Size, xy$predicted[i]) / Size

rand_df <- data.frame(X = rep(xy$X, each = 10000), Y = as.vector(rand_df))

pred_df <- data.frame(X = unique(xy$X))
pred_df$predicted <- predict(Mod, type = "response", newdata = pred_df)

    ggplot(data = xy, aes(x = factor(X), y = Y)) + geom_violin(data = rand_df, aes(x = factor(X)), adjust = 10) + geom_point(data = xy, aes(x = factor(X), y = Y)) + geom_path(data = pred_df, aes(x = factor(X), y = predicted, group = 1), color = "blue", size = 1) + geom_point(data = xy, aes(x = factor(X), y = predicted), color = "red", size = 3) + labs(x = "Предиктор", y = "Зависимая переменная \n(доля положительных исходов)") + ggtitle("Модель для бинарных данных") 

```
  





## Функция правдоподобия для биномиального распределения

Для случая биномиального распределения $x \in Bin(n, \pi)$ функция правдоподобия имеет следующий вид:

$$Lik(\pi|x) = \frac{n!}{(n-x)!x!}\pi^x(1-\pi)^{n-x}$$

отбросив константу, получаем:

$$Lik(\pi|x) \propto \pi^x(1-\pi)^{n-x}$$

### Логарифм правдоподобия

Удобнее работать с логарифмом функции правдоподобия - $logLik$ - его легче максимизировать. В случае биномиального распределения он выглядит так:

$$logLik(\pi|x) = x log(\pi) + (n-x)log(1-\pi)$$

## Подберем модель  {.smaller}

```{r}
liz_model <- glm(PA ~ PARATIO , family="binomial", data = liz)
summary(liz_model)
```

## {.smaller} 
### `summary()` для модели, подобранной методом максимального правдоподобия

```{r, echo=FALSE}
summary(liz_model)
```

Есть уже знакомые термины: `Estimate`, `Std. Error`, `AIC`  
Появились новые термины: `z value`, `Pr(>|z|)`, `Null deviance`, `Residual deviance`


## "z value"" и "Pr(>z)"

z - это величина критерия Вальда (_Wald statistic_) - аналог t-критерия

Используется для проверки $H_0: \beta_1=0$

$$z=\frac{\beta_1}{SE_{\beta_1}}$$

Сравнивают со стандартным нормальным распределением (z-распределение)

Дает надежные оценки p-value при больших выборках

## Null deviance и Residual deviance {.smaller}

Имеющиеся данные позволяют "вписать" три типа моделей  


**"Насыщенная" модель** - модель, подразумевающая, что каждая из n точек имеет свой собственный параметр, следовательно надо подобрать n параметров. Вероятность существования данных для такой модели равна 1. 
$$logLik_{satur}=0$$
$$df_{saturated} = n - npar_{saturated}  = n - n = 0$$

**"Нулевая" модель** - модель, подразумевающая, что для описания всех точек надо подобрать только 1 параметр. $g(x) = \beta_0$.  $$logLik_{nul} \ne 0$$
$$df_{null} = n - npar_{null} = n - 1$$

**"Предложенная" модель** - модель, подобранная в нашем анализе $g(x) = \beta_0 + \beta_1x$
$$logLik_{prop} \ne 0$$
$$df_{proposed} = n - npar_{proposed}$$

## Null deviance и Residual deviance

**Девианса** - это оценка отклонения логарифма максимального правдоподобия одной модели от логарифма максимального правдоподобия другой модели 

**Остаточная девианса**:   
$Dev_{resid} = 2(logLik_{satur} - logLik_{prop})=-2logLik_{prop}$    

**Нулевая девианса**:   
$Dev_{nul} = 2(logLik_{satur} - logLik_{nul})=-2logLik_{nul}$   

Проверим, совпадут ли со значениями из `summary()`
```{r}
(Dev_resid <- -2*as.numeric(logLik(liz_model))) #Остаточная девианса

(Dev_nul <- -2*as.numeric(logLik(update(liz_model, ~-PARATIO)))) #Нулевая девианса
```


## Анализ девиансы

**По соотношению нулевой девиансы и остаточной девиансы можно понять насколько статистически значима модель**

В основе анализа девиансы лежит критерий $G^2$

$$ G^2 = Dev_{nul} - Dev_{resid} = -2(logLik_{nul} - logLik_{prop})$$

```{r}
(G2 <- Dev_nul - Dev_resid)
```

Вспомним тест отношения правдоподобий: 
$$ LRT = 2ln(Lik_1/Lik_2) = 2(logLik_1 - logLlik_2)$$

> Тест $G^2$ - это частный случай теста отношения правдоподобий (Likelihood Ratio Test)

## Свойства критерия $G^2$

>- $G^2$ - это девианса полной (предложенной) и редуцированной модели (нулевой)  
>- $G^2$ - аналог частного F критерия в обычном регрессионном анализе  
>- $G^2$ - подчиняется $\chi^2$ распределению если нулевая модель и предложенная модель не отличаются друг от друга. Параметр df - разница числа параметров предложенной и нулевой моделей.
>- $G^2$ можно использовать для проверки гипотезы о равенстве нулевой и остаточной девианс.

## Задание

1. Вычислите вручную значение критерия $G^2$ для модели, описывающей встречаемость ящериц (`liz_model`) 
2. Оцените уровень значимости для него 

$$ G^2 = -2(logLik_{nul} - logLik_{prop})$$

## Решение

```{r}
#Остаточная девианса
Dev_resid <- -2*as.numeric(logLik(liz_model)) 

#Нулевая девианса
Dev_nul <- -2*as.numeric(logLik(update(liz_model, ~-PARATIO)))

# Значение критерия 
(G2 <- Dev_nul - Dev_resid)

(p_value <- 1 - pchisq(G2, df = 1))

```


## Решение с помощью функции `Anova()`

```{r}
library(car)
Anova(liz_model)


```

# Интерпретация коэффициентов логистической регрессии

## Как трактовать коэффициенты подобранной модели?

$$ g(x)=\ln(\frac{\pi(x)}{1-\pi(x)})=\beta_0 + \beta_1x$$

```{r}
coef(liz_model)
```

$\beta_0$ - не имеет особого смысла, просто поправочный коэффициент

$\beta_1$ - _на сколько_ единиц изменяется логарифм величины шансов (odds), если значение предиктора изменяется на единицу

Трактовать такую величину неудобно и трудно

## Немного алгебры

посмотрим как изменится $g(x)=\ln(\frac{\pi(x)}{1-\pi(x)})$ при изменении предиктора на 1

$$g(x+1) - g(x) = ln(odds_{x+1}) - ln(odds_x)  = ln(\frac{odds_{x+1}}{odds_x})$$

Задание: завершите алгебраическое преобразование



## Решение

$$ln(\frac{odds_{x+1}}{odds_x}) = \beta_0 + \beta_1(x+1) - \beta_0 - \beta_1x = \beta_1$$

$$ln(\frac{odds_{x+1}}{odds_x}) = \beta_1$$

$$\frac{odds_{x+1}}{odds_x} = e^{\beta_1}$$


## Полученная величина имеет определенный смысл 

```{r}
exp(coef(liz_model)[2])
```

_Во сколько_ раз изменяются шансы встретить ящерицу при увеличении отношения периметра острова к его площади на одну единицу. *NB: Отношение периметра к площади тем больше, чем меньше остров*.

Шансы изменяются в `r exp(coef(liz_model)[2])` раза. То есть, чем больше отношение  периметра к площади, тем меньше шансов встретить ящерицу. Значит, чем больше остров, тем больше шансов встретить ящерицу



## Подобранные коэффициенты позволяют построить логистическую кривую {.smaller .columns-2}


```{r, fig.height=5,fig.width=4.5,  echo=FALSE, fig.align='left'}
ggplot(liz, aes(x=PARATIO, y=PA)) + geom_point() + geom_smooth(method="glm", method.args = list(family="binomial"), se=TRUE, size = 2) + ylab("Вероятность встречи ящериц") + annotate("text", x=40, y=0.75, parse=TRUE, label = "pi == frac(e ^ {beta[0]+beta[1] %.% x}, 1 + e ^ {beta[0]+beta[1] %.% x})", size = 10)
```

Серая область - доверительный интервал для логистической регрессии

Доверительные интервалы для коэффициентов:
```{r, warning=FALSE, message=FALSE}
confint(liz_model) # для логитов
exp(confint(liz_model)) # для отношения шансов 

```


##Задание: 
Постройте график логистической регрессии для модели `liz_model`  без использования `geom_smooth()`

Hint 1: Используйте функцию `predict()`, изучите значения параметра "type"

Hint 2: Для вызова справки напишите `predict.glm()`

Hint 3: Создайте датафрейм MyData с переменной `PARATIO`, изменяющейся от минимального до максимального значения `PARATIO`

## Решение {.smaller .columns-2}

```{r, fig.height=5, fig.width=4.5, fig.align='left',echo=FALSE}
MyData <- data.frame(PARATIO = 
        seq(min(liz$PARATIO), max(liz$PARATIO)))

MyData$Predicted <- predict(liz_model, 
                            newdata = MyData, 
                            type = "response")

ggplot(MyData, aes(x = PARATIO, y = Predicted)) + 
  geom_line(size=2, color = "blue") + 
  xlab("Отношение периметра к площади") + 
  ylab ("Вероятность") + 
  ggtitle("Вероятность встречи ящериц")

```

```{r, fig.height=5, fig.width=4.5, fig.align='right',eval=FALSE}
MyData <- data.frame(PARATIO = 
        seq(min(liz$PARATIO), max(liz$PARATIO)))

MyData$Predicted <- predict(liz_model, 
                            newdata = MyData, 
                            type = "response")

ggplot(MyData, aes(x = PARATIO, y = Predicted)) + 
  geom_line(size=2, color = "blue") + 
  xlab("Отношение периметра к площади") + 
  ylab ("Вероятность") + 
  ggtitle("Вероятность встречи ящериц")

```

## Применим матричную алгебру для вычисления предсказанных значений и доверительного интервала для линии регрессии 

```{r}
# Создаем искуственный набор данных
MyData <- data.frame(PARATIO = seq(min(liz$PARATIO), max(liz$PARATIO)))

# Формируем модельную матрицу для искуственно созданных данных
X <- model.matrix( ~ PARATIO, data = MyData)

```

## Извлекаем характеристики подобранной модели и получаем предсказанные значения

```{r}
# Вычисляем параметры подобранной модели и ее матрицу ковариаций
betas    <- coef(liz_model) # Векор коэффицентов
Covbetas <- vcov(liz_model) # Ковариационная матрица

# Вычисляем предсказанные значения, перемножая модельную матрицу на вектор 
# коэффициентов
MyData$eta <- X %*% betas
```

## Получаем предсказанные значения


```{r}
# Переводим предсказанные значения из логитов в вероятности
MyData$Pi  <- exp(MyData$eta) / (1 + exp(MyData$eta))

```

## Вычисляем границы доверительного интервала

```{r}
# Вычисляем стандартные отшибки путем перемножения матриц
  MyData$se <- sqrt(diag(X %*% Covbetas %*% t(X)))

# Вычисляем доверительные интервалы
MyData$CiUp  <- exp(MyData$eta + 1.96 *MyData$se) / 
  (1 + exp(MyData$eta  + 1.96 *MyData$se))

MyData$CiLow  <- exp(MyData$eta - 1.96 *MyData$se) / 
  (1 + exp(MyData$eta  - 1.96 *MyData$se))


```

## Строим график {.columns-2} 

```{r, fig.height=5, fig.width=4.5, fig.align='left', echo=FALSE}
ggplot(MyData, aes(x = PARATIO, y = Pi)) + 
  geom_line(aes(x = PARATIO, y = CiUp), 
            linetype = 2, size = 1) + 
  geom_line(aes(x = PARATIO, y = CiLow), 
            linetype = 2, size = 1) + 
  geom_line(color = "blue", size=2) + 
  ylab("Вероятность встречи")
```


```{r, fig.height=5, fig.width=4.5, fig.align='right', echo=TRUE, eval=FALSE}
ggplot(MyData, aes(x = PARATIO, y = Pi)) + 
  geom_line(aes(x = PARATIO, y = CiUp), 
            linetype = 2, size = 1) + 
  geom_line(aes(x = PARATIO, y = CiLow), 
            linetype = 2, size = 1) + 
  geom_line(color = "blue", size=2) + 
  ylab("Вероятность встречи")
```

#Диагностика модели

##Проблема избыточности дисперсии (overdispersion)
Если данные подчиняются биномиальному распределению, 
то Среднее: &emsp;&emsp; $E(Y)  = N \times \pi$  
Дисперсия: $var(Y) = N \times \pi \times (1-\pi)$ 

То есть, дисперсия связана со средним. 
Если это нарушается, то мы не можем доверять результатам, так как модель, основанная на биномиальном распределении, применяется к данным, которые не подчиняются этому распределению. 


##Проблема избыточности дисперсии (overdispersion)

Причины изботочности дисперсии 

- При одном и том же значении предиктора вероятности событий не одинаковы (not identically distributed)

- События зависимы друг от друга (события при одном значении предиктора зависят от событий при другом зачении предиктора)


  

##Проверка на избыточность дисперсии

Пирсоновские остатки $E_i = \frac{\varepsilon_i}{\sqrt{Var(\hat{y_i})}}$

```{r}
EP <- resid(liz_model, type = "pearson") #Пирсоновские остатки
p <- length(coef(liz_model)) #число параметров в модели  
df <-   nrow(model.frame(liz_model)) - p #число степеней свободы
Overdisp <- sum(EP^2) / df

Overdisp
```
Здесь проблем нет!

Как поступать в случае присутствия избыточности дисперсии будет отдельный разговор.


##Есть ли паттерн в остатках?

В случае бинарных переменных появляется некоторая проблема...
```{r}

library(ggplot2)
liz_mod_diagn <- fortify(liz_model)
ggplot(liz_mod_diagn, aes(x = .fitted, y = .stdresid)) + geom_point()



```


##Есть ли паттерн в остатках?
Для оценки присутствия паттерна надо разделить все .fitted на группы и посчитать среденее значение остатков

```{r}
library(dplyr)
liz_mod_diagn$group <- ntile(liz_mod_diagn$.fitted, 8)

resi_and_fit <- liz_mod_diagn %>%  group_by(group) %>%  summarise(mean_fit = mean(.fitted), mean_res = mean(.stdresid))

qplot(resi_and_fit$mean_fit, resi_and_fit$mean_res ) + geom_smooth(method = "lm")


```





# Множественная логистическая регрессия


## От чего зависит уровень смертности пациентов, выписанных из реанимации? {.smaller}

Данные, полученные на основе изучения 200 историй болезни пациентов одного из американских госпиталей        

<div class="columns-2">

- STA: Статус (0 = Выжил, 1 = умер)  
- AGE: Возраст  
- SEX: Пол  
- RACE: Раса  
- SER: Тип мероприятий в реанимации (0 = Medical, 1 = Surgical)  
- CAN: Присутствует ли онкология? (0 = No, 1 = Yes)  
- CRN: Присутствует ли почечная недостаточность (0 = No, 1 = Yes)  
- INF: Возможность инфекции (0 = No, 1 = Yes)  
- CPR: CPR prior to ICU admission (0 = No, 1 = Yes)  
- SYS: Давление во время поступления в реанимацию (in mm Hg)  
- HRA: Пульс (beats/min)

<p />

- PRE: Была ли госпитализация в предыдущие 6 месяцев (0 = No, 1 = Yes)  
- TYP: Тип госпитализации (0 = Elective, 1 = Emergency)  
- FRA: Присутствие переломов (0 = No, 1 = Yes)  
- PO2: Концентрация кислорода в крови (0 = >60, 1 = ²60)  
- PH: Уровень кислотности крови (0 = ³7.25, 1 < 7.25)  
- PCO: Концентрация углекислого газа в крови (0 = ²45, 1 = > 45)  
- BIC: Bicarbonate from initial blood gases (0 = ³18, 1 = < 18)  
- CRE: Уровень креатина (0 = ²2.0, 1 = > 2.0)  
- LOC: Уровень сознания пациента при реанимации (0 = no coma or stupor, 1= deep stupor, 2 = coma)  

</div>

## Смотрим на данные {.smaller}

```{r}
surviv <- read.table("data/ICU.csv", header=TRUE, sep=";")
head(surviv)
```

##Сделаем факторами те дискретные предикторы, которые обозначенны цифрами
```{r}
surviv$PO2 <- factor(surviv$PO2)
surviv$PH <- factor(surviv$PH)
surviv$PCO <- factor(surviv$PCO)
surviv$BIC <- factor(surviv$BIC)
surviv$CRE <- factor(surviv$CRE)
surviv$LOC <- factor(surviv$LOC)
```



## Строим модель {.smaller}
```{r, warning=FALSE}
M1 <- glm(STA ~ ., family = "binomial", data = surviv)
summary(M1)
```


##Задание
Проведите анализ девиансы для данной модели


##Решение {.smaller}
```{r}
Anova(M1)

```


##Задание
Подберите оптимальную модель в соответствии с протоколом обратного пошагового отбора


##Решение 

```{r}
drop1(M1, test = "Chi")
M2 <- update(M1, .~.-CRE)
```


##Решение 

```{r}
drop1(M2, test = "Chi")
M3 <- update(M2, .~.-CRN)
```



##Решение 

```{r}
drop1(M3, test = "Chi")
M4 <- update(M3, .~.-INF)
```



##Решение 

```{r}
drop1(M4, test = "Chi")
M5 <- update(M4, .~.-BIC)
```


##Решение 

```{r}
drop1(M5, test = "Chi")
M6 <- update(M5, .~.-PO2)
```



##Решение 

```{r}
drop1(M6, test = "Chi")
M7 <- update(M6, .~.-HRA)
```



##Решение 

```{r}
drop1(M7, test = "Chi")
M8 <- update(M7, .~.-SER)
```


##Решение 

```{r}
drop1(M8, test = "Chi")
M9 <- update(M8, .~.-FRA)
```



##Решение 

```{r}
drop1(M9, test = "Chi")
M10 <- update(M9, .~.-RAC)
```



##Решение 

```{r}
drop1(M10, test = "Chi")
M11 <- update(M10, .~.-SEX)
```


##Решение 

```{r}
drop1(M11, test = "Chi")
M12 <- update(M11, .~.-CPR)
```




##Решение 

```{r}
drop1(M12, test = "Chi")
M13 <- update(M12, .~.-PRE)
```


##Решение 

```{r}
drop1(M13, test = "Chi")
M14 <- update(M13, .~.-PH)
```



##Решение 

```{r}
drop1(M14, test = "Chi")
M15 <- update(M14, .~.-PCO)
```

##Решение 

```{r}
drop1(M15, test = "Chi")
```


##Рассмотрим финальную модель {.smaller}

M15 вложена в M1 следовательно их можно сравнить тестом отношения правдоподобий


```{r}
anova(M1, M15, test = "Chi")
```


## Диагностика модели {.smaller}

```{r, message=FALSE}
M15_diag <- data.frame(.fitted = predict(M15), 
    .pears_resid = residuals(M15, type = "pearson"))

M15_diag$group <- ntile(M15_diag$.fitted, 10)


resi_and_fit <- M15_diag %>%  group_by(group) %>%  summarise(mean_fit = mean(.fitted), mean_res = mean(.pears_resid))


ggplot(fortify(M15), aes(x = .fitted, y = .stdresid)) + geom_point() +  geom_smooth()

```

Явного паттерна в остатках нет

##Проверяем на избыточность дисперсии
```{r}
E <- resid(M15, type = "pearson") #Пирсоновские остатки
p <- length(coef(M15)) #число параметров в модели  
df <-   nrow(model.frame(M15)) - p #число степеней свободы
Overdisp <- sum(E^2) / df

Overdisp
```

Признаков избыточности дисперсии нет!




## Вопрос 
Во сколько раз изменяется отношение шансов на выживание при условии, что пациент онкологический больной (при прочих равных условиях)?


## Решение

```{r}
exp(coef(M15)[3])
```

##Визуализируем предсказания модели

```{r, echo=FALSE, fig.height=6, fig.width=8}
MyData = expand.grid(AGE = seq(min(surviv$AGE), max(surviv$AGE), 1), CAN = levels(surviv$CAN),  SYS = seq(min(surviv$SYS), max(surviv$SYS), 10),  LOC =  levels(surviv$LOC), TYP = "Emergency") 

MyData$Predicted <- predict(M15, newdata = MyData, type = "response")



ggplot(MyData, aes(x=SYS, y = Predicted, color = AGE, group = AGE)) + geom_line() + facet_grid(LOC~ CAN, labeller = label_both) + scale_color_gradient(low = "green",  high = "red") + labs(label = list(x = "Давление в момент реанимации (SYS)", y = "Вероятность гибели", color = "Возраст", title = "Предсказания модели"))


```



##Код для графика

```{r, eval=F}
MyData = expand.grid(AGE = seq(min(surviv$AGE), max(surviv$AGE), 1), 
                     CAN = levels(surviv$CAN),  
                     SYS = seq(min(surviv$SYS), max(surviv$SYS), 10),  
                     LOC =  levels(surviv$LOC), 
                     TYP = "Emergency") 

MyData$Predicted <- predict(M15, newdata = MyData, type = "response")



ggplot(MyData, aes(x=SYS, y = Predicted, color = AGE, group = AGE)) + 
  geom_line() + 
  facet_grid(LOC~ CAN, labeller = label_both) + 
  scale_color_gradient(low = "green",  high = "red") + 
  labs(label = list(x = "Давление в момент реанимации (SYS)", y = "Вероятность гибели", color = "Возраст", title = "Предсказания модели"))

```



## Summary

>- При построении модели для бинарной зависимой перменной применяется логистическая регрессия.   
>- При построении такой модели 1 и 0 в перменной отклика заменяются логитами.
>- Угловые коэффициенты подобранной логистической регрессии говорят о том, во сколько раз изменяется соотношение шансов события при увеличении предиктора на единицу.   
>- Оценить статистическую значимость модели можно с помощью анализа девиансы.


## Что почитать
+ Кабаков Р.И. R в действии. Анализ и визуализация данных на языке R. М.: ДМК Пресс, 2014.
+ Quinn G.P., Keough M.J. (2002) Experimental design and data analysis for biologists, pp. 92-98, 111-130
+ Zuur, A.F. et al. 2009. Mixed effects models and extensions in ecology with R. - Statistics for biology and health. Springer, New York, NY. 


