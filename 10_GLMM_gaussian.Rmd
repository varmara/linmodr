---
title: "Смешанные линейные модели"
subtitle: "Линейные модели..."
author: "Марина Варфоломеева, Вадим Хайтов"
output:
  beamer_presentation:
    colortheme: beaver
    highlight: tango
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - --latex-engine=xelatex
    - -V fontsize=10pt
    - -V lang=russian
    slide_level: 2
    theme: default
    toc: no
institute: "СПбГУ"
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# to render
# rmarkdown::render("10_GLMM_gaussian.Rmd", output_format = "beamer_presentation")
# options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
# library("extrafont")
source("support_linmodr.R")
```

## Вы узнаете

- Что такое смешаные модели и когда они применяются
- Что такое фиксированные и случайные факторы

### Вы сможете

- Рассказать чем фиксированные факторы отличаются от случайных
- Привести примеры факторов, которые могут быть фиксированными или случайными в зависимости от задачи исследования
- Рассказать, что оценивает коэффициент внутриклассовой корреляции и вычислить его для  случая с одним случайным фактором
- Подобрать смешаную линейную модель со случайным отрезком и случайным углом наклона в R при помощи методов максимального правдоподобия

# "Многоуровневые" данные

## Пример: Как время реакции людей зависит от бессонницы?

В статье  Belenky et al., 2003. приводится такая схема исследования:

\includegraphics[height=0.5\paperheight]{images/Belenky_et_al._2003_fig_1.png}

В датасете `sleepstudy` из пакета `lme4` описание немного отличается от того, что в статье:
В ночь перед нулевым днем всем испытуемым давали поспать нормальное время, а в следующие 9 ночей --- давали спать по 3 часа. Каждый день измеряли время реакции в серии тестов. 

\tiny

Данные: Belenky et al. (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.


## Данные `sleepstudy`

- `Reaction` --- среднее время реакции в серии тестов в день наблюдения, мс
- `Days` --- число дней депривации сна
- `Subject` --- номер испытуемого

```{r}
library(lme4)
data(sleepstudy)
sl <- sleepstudy
head(sl, 3)
```

## Знакомство с данными

```{r}
str(sl)
# пропущенные значения
colSums(is.na(sl))
```

## Знакомство с данными

```{r}
# число субъектов
length(unique(sl$Subject))
# сбалансирован ли объем выборки?
table(sl$Subject)
table(sl$Subject, sl$Days)
```

## Есть ли выбросы?

```{r}
library(ggplot2)
theme_set(theme_bw())
# построим дот-плот
ggplot(sl, aes(x = Reaction, y = 1:nrow(sl))) +
  geom_point()
```
\pause

Кажется, что нет ничего странного, но мы еще не учли информацию о субъектах

## Как меняется время реакции разных субъектов?

```{r}
ggplot(sl, aes(x = Reaction, y = Subject, colour = Days)) +
  geom_point() 
```

\pause

- Видно, что у разных субъектов время реакции различается. Есть быстрые, есть медленные, кого-то недосып стимулирует. Сама по себе межиндивидуальная изменчивость нас не интересует, но ее нельзя игнорировать.

## Что делать с разными субъектами?

\pause

\begin{minipage}{\linewidth}
\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.4\textwidth}
\includegraphics{images/the_good1.jpg}
\end{wrapfigure}
The Good --- подбираем смешанную модель, в которой есть фиксированный фактор `Days` и случайный фактор `Subject`, который опишет межиндивидуальную изменчивость.
\end{minipage}

\vspace{12mm}

\pause

\begin{minipage}{\linewidth}
\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.4\textwidth}
\includegraphics{images/the_bad1.jpg}
\end{wrapfigure}
The Bad --- игнорируем структуру данных, подбираем модель с единственным фиксированным фактором `Days`. (Не учитываем группирующий фактор `Subject`). Неправильный вариант.
\end{minipage}

\pause

\vfill

\begin{minipage}{\linewidth}
\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.4\textwidth}
\includegraphics{images/the_ugly1.jpg}
\end{wrapfigure}
The Ugly --- подбираем модель с двумя фиксированными факторами: `Days` и `Subject`. (Группирующий фактор `Subject` опишет межиндивидуальную изменчивость как обычный фиксированный фактор).
\end{minipage}

## The Bad. Не учитываем группирующий фактор.

$$Reaction_{i} = \beta_0 + \beta_1 Days_{i} + \varepsilon_{i}$$

$\varepsilon_i \sim N(0, \sigma^2)$  
$i = 1, 2, ..., 180$ -- общее число наблюдений

В матричном виде 
$$\mathbf{Reaction} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$$


```{r}
Wrong1 <- lm(Reaction ~ Days, data = sl)
```

График этой модели

```{r echo=FALSE}
ggplot(sl, aes(x = Days, y = Reaction)) +
  geom_point() +
  geom_smooth(se = TRUE, method = "lm", size = 1)
```


## The Bad. Не учитываем группирующий фактор.

\small

```{r}
summary(Wrong1)
```

\pause

\normalsize

- Если мы не учитываем группирующий фактор, увеличивается вероятность ошибок I рода. Все будет казаться "очень достоверно" из-за низких стандартных ошибок. Но поскольку в этом случае условие независимости нарушено --- __все не так, как кажется__.

## The Ugly. Группирующий фактор как фиксированный.

$$Reaction_{ij} = \beta_0 + \beta_1 Days_{j} + \beta_{2}Subject_{i = 2} + ... + \beta_{2}Subject_{i = `r length(unique(sl$Subject))`} + \varepsilon_{ij}$$

$\varepsilon_{ij} \sim N(0, \sigma^2)$ - остатки от регрессии  
$i = 1, 2, ..., 18$ - субъект  
$j = 1, 2, ..., 10$ - день

В матричном виде
$$\mathbf{Reaction} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$$

```{r}
Wrong2 <- lm(Reaction ~ Days + Subject, data = sl)
```

\pause

Если мы учитываем группирующий фактор как обычно (как __фиксированный фактор__), придется оценивать слишком много параметров (`r length(unique(sl$Subject))` для уровней группирующего фактора, 1 для `Days`, $\sigma$ --- всего `r length(coef(Wrong2)) + 1`).
При этом у нас всего `r sum(complete.cases(sl))` наблюдений. Чтобы получить удовлетворительную мощность, нужно минимум 10--20 наблюдений на каждый параметр (Harrell, 2013) --- у нас `r sum(complete.cases(sl))/(length(coef(Wrong2)) + 1)`.

## The Ugly. Что нам делать с этим множеством прямых?

```{r}
Wrong2_diag <- fortify(Wrong2)
ggplot(Wrong2_diag, aes(x = Days, colour = Subject)) +
  geom_line(aes(y = .fitted, group = Subject)) +
  geom_point(data = sl, aes(y = Reaction)) +
  guides(colour = guide_legend(ncol = 2))
```

\pause

В этой модели, где субъект --- это фиксированный фактор, для каждого субъекта есть "поправка" для значения свободного члена в уравнении регрессии. В результате универсальность модели теряется: предсказания можно сделать только на индивидуальном уровне --- с учетом субъекта

# Фиксированные и случайные факторы

## Можно посмотреть на группирующий фактор иначе!

Когда нам не важны конкретные значения интерсептов для разных уровней фактора, мы можем представить, что эффект фактора (величина "поправки") --- случайная величина, и можем оценить дисперсию между уровнями группирующего фактора.

Такие факторы называются __случайными факторами__, а модели с такими факторами называются __смешанными моделями__:

- Общие смешанные модели (general linear mixed models) --- нормальное распределение зависимой переменной

- Обобщенные смешанные модели (generalized linear mixed models) --- другие формы распределений зависимой переменной

## Фиксированные и случайные факторы

\resizebox{1\textwidth}{!}{
\begin{tabular}{L{0.2\textwidth} C{0.4\textwidth} C{0.4\textwidth}}
\hline\noalign{\smallskip}
Свойства & Фиксированные факторы & Случайные факторы \\
\hline\noalign{\smallskip}
Уровни фактора & фиксированные, заранее определенные и потенциально воспроизводимые уровни & случайная выборка из всех возможных уровней \\
Используются для тестирования гипотез & о средних значениях отклика между уровнями фактора \linebreak $H _{0}: \mu _1 = \mu _2 = \ldots = \mu _i = \mu$ & о дисперсии отклика между уровнями фактора \linebreak $H _{0}: \sigma_{rand.fact.}^2 = 0$ \\
Выводы можно экстраполировать & только на уровни из анализа & на все возможные уровни \\
Число уровней фактора & Осторожно! Если уровней фактора слишком много, то нужно подбирать слишком много коэффициентов --- должно быть много данных & Важно! Для точной оценки $\sigma$ нужно нужно много уровней фактора --- не менее 5 \\
\hline\noalign{\smallskip}
\end{tabular}
}

## Примеры фиксированных и случайных факторов

__Фиксированные факторы__

- Пол
- Низина/вершина
- Илистый/песчаный грунт
- Тень/свет
- Опыт/контроль

__Случайные факторы__

- Субъект, особь или площадка (если есть несколько измерений)
- Выводок (птенцы из одного выводка имеют право быть похожими)
- Блок, делянка на участке
- Аквариум в лаб. эксперименте

## Задание 1

Какого типа эти факторы? Поясните ваш выбор.

- Несколько произвольно выбранных градаций плотности моллюсков в полевом эксперименте, где плотностью манипулировали.

- Фактор размер червяка (маленький, средний, большой) в выборке червей.

- Деление губы Чупа на зоны с разной степенью распреснения.

# Cмешанные линейные модели

## Cмешанная линейная модель в общем виде

$$\mathbf{Y}_i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z}_i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon} _i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$ --- случайные эффекты нормально распределены со средним 0 и матрицей ковариаций $\mathbf{D}$ (дисперсией $\sigma_{b}^2$)

$\boldsymbol{\varepsilon} _i \sim N(0, \boldsymbol{\Sigma})$ --- остатки модели нормально распределены со средним 0 и матрицей ковариаций $\boldsymbol{\Sigma}_i$ (дисперсией $\sigma^2$)

$\mathbf{X} _i \cdot \boldsymbol{\beta}$ --- фиксированная часть модели

$\mathbf{Z}_i \cdot \mathbf{b} _i$ --- случайная часть модели

## В примере модель со случайным отрезком можно записать так:

$$Reaction_{ij} = \beta_0 + \beta_1 Days_{ij} + b_i + \varepsilon_{ij}$$

$b_{i} \sim N(0, \sigma_b^2)$ --- случайный эффект субъекта (intercept)  
$\varepsilon_{ij} \sim N(0, \sigma^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни

\pause

Для каждого субъекта $i$ в матричном виде это записывается так:

$$\begin{pmatrix} Reaction _{i1} \\ Reaction _{i2} \\ \vdots \\ Reaction _{i10} \end{pmatrix} 
= \begin{pmatrix}
1 & Days _{i1} \\ 1 & Days _{i2} \\ \vdots \\ 1 & Days _{i10}
\end{pmatrix} 
\cdot
 \begin{pmatrix}
\beta _0 \\ \beta _1
\end{pmatrix}  +
 \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix} 
\cdot b _{i} +
 \begin{pmatrix} \varepsilon _{i1} \\ \varepsilon _{i2}\\ \vdots \\ \varepsilon _{i10} \end{pmatrix} $$

\pause

что можно записать сокращенно так:

$$\mathbf{Reaction} _i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z} _i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon}_i$$





## Теперь разберемся с допущениями модели

$$\mathbf{Reaction} _i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z} _i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon}_i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$ - случайные эффекты $b _i$ нормально распределены со средним 0 и матрицей ковариаций $\mathbf{D}$  
$\boldsymbol{\varepsilon} _i \sim N(0, \boldsymbol{\Sigma} _i)$ - остатки модели нормально распределены со средним 0 и матрицей ковариаций $\boldsymbol{\Sigma} _i$

\pause

Матрица ковариаций остатков для каждого субъекта выглядит так:
$$\boldsymbol{\Sigma} _i = \sigma^2 \cdot
 \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix} $$

\pause

Т.е. остатки независимы друг от друга (вне диагонали стоят нули, т.е. ковариация разных остатков 0).

В то же время, отдельные значения переменной-отклика $\mathbf{Y} _i$ уже не будут независимы друг от друга при добавлении случайных эффектов - см. ниже

## Матрица ковариаций переменной-отклика

$$\mathbf{Reaction} _i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z} _i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon}_i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$  
$\boldsymbol{\varepsilon} _i \sim N(0, \boldsymbol{\Sigma} _i)$


Можно показать, что переменная-отклик $\mathbf{Y} _i$ нормально распределена 

$\mathbf{Y} _i \sim N(\mathbf{X} _i \cdot \boldsymbol{\beta}, \mathbf{V} _i )$

\pause

Матрица ковариаций переменной-отклика:

$$\mathbf{V} _i = \mathbf{Z} _i \mathbf{D} \mathbf{Z'} _i + \boldsymbol{\Sigma} _i$$

где $\mathbf{D}$ --- матрица ковариаций случайных эффектов.

Т.е. __добавление случайных эффектов приводит к изменению ковариационной матрицы__ $\mathbf{V} _i$

\vfill
\footnotesize

Кстати, $\mathbf{Z} _i \mathbf{D} \mathbf{Z'} _i$ называется преобразование Холецкого (Cholesky decomposition)

## Добавление случайных эффектов приводит к изменению ковариационной матрицы

$$\mathbf{V} _i = \mathbf{Z} _i \mathbf{D} \mathbf{Z'} _i + \boldsymbol{\Sigma} _i$$

Для простейшей смешанной модели со случайным отрезком:

$$\mathbf{V} _i =  \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}
\cdot \sigma_b^2
\cdot  \begin{pmatrix} 1 & 1 & \cdots & 1 \end{pmatrix} +
\sigma^2
\cdot
 \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix}  =$$

$$
=  \begin{pmatrix}
\sigma^2 + \sigma_b^2 & \sigma_b^2 & \cdots & \sigma_b^2 \\
\sigma_b^2 & \sigma^2 + \sigma_b^2 & \cdots & \sigma_b^2 \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_b^2 & \sigma_b^2 & \sigma_b^2 & \sigma^2 + \sigma_b^2
\end{pmatrix} 
$$

## Индуцированная корреляция - следствие  включения в модель случайных эффектов
$$\mathbf{V} _i =
 \begin{pmatrix}
\sigma^2 + \sigma_b^2 & \sigma_b^2 & \cdots & \sigma_b^2 \\
\sigma_b^2 & \sigma^2 + \sigma_b^2 & \cdots & \sigma_b^2 \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_b^2 & \sigma_b^2 & \sigma_b^2 & \sigma^2 + \sigma_b^2
\end{pmatrix}  
$$

\pause

$\sigma_b^2$ --- ковариация между наблюдениями одного субъекта  
$\sigma^2 + \sigma_b^2$ --- дисперсия

Т.е. корреляция между наблюдениями одного субъекта $\sigma_b^2 / (\sigma^2 + \sigma_b^2)$

\pause

### Коэффициент внутриклассовой корреляции $\sigma_b^2 / (\sigma^2 + \sigma_b^2)$

Способ измерить, насколько коррелируют друг с другом наблюдения из одной и той же группы случайного фактора. Если он высок, то можно брать меньше проб в группе (и больше групп, если нужно)

# Подбор смешанных моделей в R

## Подбор смешанных моделей в R

Самые популярные пакеты --- `nlme` (старый, иногда медленный, стабильный, хорошо документированный) и `lme4` (новый, быстрый, не такой стабильный, хуже документированный). Есть много других.

\resizebox{1\textwidth}{!}{
\begin{tabular}{L{0.2\textwidth} C{0.2\textwidth} C{0.2\textwidth} C{0.2\textwidth} C{0.2\textwidth}}
\hline\noalign{\smallskip}
Функция     & lme() из nlme & lmer() из lme4 & glmer() из lme4 & glmmPQL() из MASS \\
\hline\noalign{\smallskip}
Распределение отклика & нормальное  & нормальное & биномиальное, пуассоновское, гамма, (+ квази) & биномиальное, пуассоновское, гамма, (+ квази), отр. биномиальное \\
Метод оценивания & ML, REML & ML, REML & ML, REML & PQL \\
Гетерогенность дисперсий & + & - & - & - \\
Корреляционные структуры & + & - & - & + \\
Доверительная вероятность (p-value) & + & - & - & + \\
\hline\noalign{\smallskip}
\end{tabular}
}

## Синтаксис для смешанных моделей в R

__Фиксированная часть модели__ задается обычной двухсторонней формулой

`Y ~ 1 + X1 + ... + Xn`

__Случайная часть модели__ - односторонняя формула. До вертикальной черты --- перечислены факторы, влияющие на случайный угол наклона. После вертикальной черты --- факторы, влияющие на случайный intercept.

`~ 1 + X1 + ... + Xn |A`

Вложенные друг в друга факторы указываются от крупного к мелкому через "/"

`~ 1 + X1 + ... + Xn |A/B/C`

Детали синтаксиса разных функций отличаются (см. следующий слайд с примерами формул)

## Синтаксис некоторых смешанных моделей

\small
\resizebox{1\linewidth}{!}{
\begin{tabular}{L{0.35\textwidth} L{0.37\textwidth} L{0.28\textwidth}}
\hline
Факторы & lme() из nlme & lmer() из lme4 \\
\hline
А -- случ. intercept & lme(fixed=Y$\sim$1,random=$\sim$1|A, data=dt) & lmer(Y$\sim$1+(1|A), data=dt) \\
A -- случ. intercept, \linebreak X -- фикс. & lme(fixed=Y$\sim$X,random=$\sim$1|A, data=dt) & lmer(Y$\sim$X+(1|A), data=dt) \\
A -- случ. intercept, \linebreak X -- случ. угол накл. & lme(fixed=Y$\sim$X,random=$\sim$1+X|A, data=dt) & lmer(Y$\sim$X+(1+X|A), data=dt) \\
%A -- случ. intercept, \linebreak X -- фикс. \linebreak A вложен в фикс.Х & nlme(fixed=Y$\sim$X,random=$\sim$1|X/A, data=dt) & lmer(Y$\sim$X+(1|A:X), data=dt) \\
A и В -- случ. intercept, \linebreak  A и B независимы (crossed effects), \linebreak X -- фикс. & & lmer(Y$\sim$X+(1|A)+(1|B), data=dt) \\
A и В -- случ. intercept, \linebreak B вложен в А (nested effects), уровни B повт. в группах по фактору A, \linebreak X -- фикс. & lme(fixed=Y$\sim$X,random=$\sim$1|A/B, data=dt) & lmer(Y$\sim$X+(1|A/B), data=dt) \linebreak lmer(Y$\sim$X+(1|A)+(1|A:B), data=dt) \\
A и В -- случ. intercept, \linebreak B вложен в А (nested random effects), все уровни B уникальны, \linebreak X -- фикс. & lme(fixed=Y$\sim$X,random=$\sim$1|A/B, data=dt) & lmer(Y$\sim$X+(1|A)+(1|B), data=dt) \\
\hline
\end{tabular}
}

# Смешанные модели со случайным отрезком в R

## Подберем модель со случайным отрезком с помощью `lme()` из пакета `nlme`. 

Функция `lme()` из пакета `nlme` нам понадобится на следующем занятии, поэтому нужно освоить ее синтаксис.

```{r}
# выгружаем lme4, чтобы не было конфликтов с nlme
detach(name = "package:lme4") 
library(nlme)
M1 <- lme(Reaction ~ Days, random = ~ 1 | Subject, data = sl)
```

Что дальше?

# Анализ остатков

## График остатков от предсказанных значений

```{r}
M1_diag <- data.frame(sl, 
                      .resid = resid(M1, type = "pearson"), 
                      .fitted <- fitted(M1))
gg_resid <- ggplot(M1_diag, aes(y = .resid)) +
  guides(colour = guide_legend(ncol = 2))
gg_resid + geom_point(aes(x = .fitted, colour = Subject))
```

\pause

- Есть большие остатки, гетерогенность дисперсий

## Графики остатков от ковариат в модели и не в модели

```{r}
library(gridExtra)
grid.arrange(gg_resid + geom_boxplot(aes(x = factor(Days))),
             gg_resid + geom_boxplot(aes(x = Subject)),
             ncol = 2, widths = c(0.4, 0.6))
```

\pause

- Большие остатки у наблюдений для 332 субъекта
- Гетерогенность дисперсий
- Пока оставим все как есть

# Тестирование гипотез в смешанных моделях

## Способы тестирования влияния факторов в смешанных моделях

Достаточно __одного__ из этих равноправных вариантов.

Важно, каким именно способом (ML или REML) подобрана модель.

(а) t-(или -z) тесты  --- приблизительный результат (REML)

(б) F-тест --- приблизительный результат (REML)

(в) Попарное сравнение вложенных моделей при помощи тестов отношения правдоподобий  (ML)

(г) Сравнение моделей по AIC (ML)

## (а) t-(или -z) тесты (REML)

- `summary(model)` 
- Подходит для непрерывных переменных или факторов с 2 уровнями.  
- Дает приблизительный результат, лучше так не делать.

\footnotesize

```{r}
summary(M1)
```

## (б) F-тест (REML)

- `anova()`
- Приблизительный результат, лучше так не делать.
- Последовательное тестирование гипотез (Type I SS) --- будьте внимательны при интерпретации

```{r}
library(car)
anova(M1, test = "F")
```

\pause

- Время реакции зависит от продолжительности бессонницы ($F_{1, 161} = 169$, $p < 0.01$)

## (в) Попарное сравнение вложенных моделей при помощи тестов отношения правдоподобий (ML)

Дает более точные выводы, чем F и t(z)

Обязательно method = "ML", а не "REML"

\footnotesize

```{r}
M1.ml <- lme(Reaction ~ Days, random = ~1|Subject, data = sl, method = "ML")
M2.ml <- lme(Reaction ~ 1, random = ~1 | Subject, data = sl, method = "ML")
```
\footnotesize

Любой из этих вариантов:

- `anova(model1, model2)`
- `drop1()`
- `Anova()` из пакета `car` --- Type II, III SS, не приводится значение отношения правдоподобий

```{r}
anova(M1.ml, M2.ml)
```

\pause

- Время реакции меняется в зависимости от продолжительности бессонницы (L = 116, df = 1, p < 0.01)

## (г) Сравнение моделей по AIC (ML)

Обязательно method = "ML", а не "REML"

```{r}
AIC(M1.ml, M2.ml)
```

\pause

- Продолжительность бессонницы влияет на время реакции (AIC)

# Подбор оптимальной модели и проверка условий применимости

## Подбор оптимальной модели и проверка условий применимости

Если вы решили подбирать оптимальную модель и выкидывать какие-то предикторы, то вам нужно будет сделать анализ остатков финальной модели.

В нашем случае модель не изменилась, поэтому данный этап выпадает из анализа

# Представление результатов

## Представление результатов

REML оценка параметров более точна (оценка случайных факторов)

Для представления результатов лучше использовать модель, подобранную при помощи Restricted Maximum Likelihood.

```{r}
M1_fin <- lme(Reaction ~ Days, random = ~1|Subject, data = sl,
              method = "REML")
```

В данном случае, этот шаг избыточен, т.к. lme использует REML по-умолчанию, и поэтому сейчас нам не нужно было ничего менять.

Но lmer использует ML, и тогда точно нужно переподобрать финальную модель при помощи REML.

## Уравнение модели

$$Reaction_{ij} = `r round(fixef(M1_fin), 1)[1]` + `r round(fixef(M1_fin), 1)[2]` Days_{ij} + b_i + \varepsilon_{ij}$$

$b_{i} \sim N(0, `r round(as.numeric(VarCorr(M1_fin)[2, 2]), 1)`^2)$ --- случайный эффект субъекта  
$\varepsilon_{ij} \sim N(0, `r round(as.numeric(VarCorr(M1_fin)[1, 2]), 1)`^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни

\vfill
\small

```{r}
fixef(M1_fin)    # Фиксированные эффекты
VarCorr(M1_fin)  # Случайные эффекты
```

## Внутриклассовая корреляция

$\sigma_{effect}^2 / (\sigma_{effect}^2 + \sigma^2)$

```{r}
# Внутриклассовая корреляция
37.12383^2 / (37.12383^2 + 30.99123^2)
```

\small
\vfill

```{r, eval=FALSE}
M1_fin
```

    В результатах
    Random effects:
     Formula: ~1 | Subject
            (Intercept) Residual
    StdDev:    37.12383 30.99123


\pause

- Значения времени реакции одного субъекта похожи. Высокая внутриклассовая корреляция показывает, что эффект субъекта нельзя игнорировать в анализе.


## Данные для графика предсказаний фиксированной части модели

```{r}
# Исходные данные
library(plyr)
NewData_M1 <- ddply(
  sl, .(Subject), summarise,
  Days = seq(min(Days), max(Days), length = 10)
  )

# Предсказанные значения при помощи predict()
# level = 0 - для фиксированных эффектов (т.е. без учета субъекта)
NewData_M1$fitted <- predict(M1_fin, NewData_M1, level = 0)

# Предсказанные значения при помощи матриц
X <- model.matrix(~ Days, data = NewData_M1)
betas <- fixef(M1_fin)
NewData_M1$fitted <- X %*% betas

# Cтандартные ошибки и дов. интервалы
NewData_M1$se <- sqrt( diag(X %*% vcov(M1_fin) %*% t(X)) )
NewData_M1$lwr <- NewData_M1$fitted - 1.98 * NewData_M1$se
NewData_M1$upr <- NewData_M1$fitted + 1.98 * NewData_M1$se
```

## График предсказаний фиксированной части модели

```{r}
ggplot(data = NewData_M1, aes(x = Days, y = fitted)) +
  geom_ribbon(alpha = 0.35, aes(ymin = lwr, ymax = upr)) +
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))
```

## Данные для графика предсказаний для индивидуальных уровней случайного фактора

Если вам любопытно, куда делась информация о разных субъектах, то вот она...

Можно получить предсказания для каждого субъекта

$\beta_0 + \beta_1 \cdot Days_{ij} + b_i$

```{r}
NewData_M1$fit_subj <- predict(M1_fin, NewData_M1, level = 1)
# или то же самое при помощи матриц
# случайные эффекты для каждого субъекта
# это датафрейм с одним столбцом
rand <- ranef(M1_fin)
# "разворачиваем" для каждой строки данных
all_rand <- rand[as.numeric(NewData_M1$Subject), 1]
# прибавляем случайные эффекты к предсказаниям фикс. части
NewData_M1$fit_subj <- X %*% betas + all_rand
```


## График предсказаний для индивидуальных уровней случайного фактора


```{r gg_M1_subj}
ggplot(NewData_M1, aes(x = Days, y = fit_subj, group = Subject)) +
  geom_ribbon(alpha = 0.5, aes(fill = Subject, ymin = fit_subj - 1.98*se, 
                  ymax = fit_subj + 1.98*se)) + 
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))  +
  guides(fill = guide_legend(ncol = 2))
```
\pause

Не факт, что на самом деле время реакции разных субъектов меняется параллельно

# Смешанные модели со случайным отрезком и углом наклона в R

## Смешанная модель со случайным отрезком и углом наклона

На графике индивидуальных эффектов было видно, что измерения для разных субъектов, возможно, идут непараллельными линиями. Усложним модель --- добавим случайные изменения угла наклона для каждого из субъектов.

Это можно биологически объяснить. Возможно, в зависимости от продолжительности бессонницы у разных субъектов скорость реакции будет ухудшаться разной скоростью: одни способны выдержать 9 дней почти без потерь, а другим уже пары дней может быть достаточно.

## Уравнение модели со случайным отрезком и углом наклона

$$Reaction_{ij} = \beta_0 + \beta_1 Days_{ij} + b_i + c_{ij} Days_{ij} + \varepsilon_{ij}$$
  
$b_{i} \sim N(0, \sigma_b^2)$ --- случайный интерсепт для субъекта  
$c_{ij} \sim N(0, \sigma_c^2)$ --- случайный угол наклона для субъекта  
$\varepsilon_{ij} \sim N(0, \sigma^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни


## Дальнейшие действия по прежнему плану:

- Подбираем модель
- Анализ остатков
- Проверка влияния факторов + подбор оптимальной модели
- Анализ остатков финальной модели
- Подбор финальной модели при помощи REML
- Описание результатов
- Визуализация предсказаний

## Смешанная модель со случайным отрезком и углом наклона

Формат записи формулы для случайных эффектов в `lme()`:

    random = ~ 1 + Угол наклона | Интерсепт


```{r}
MS1 <- lme(Reaction ~ Days, random = ~ 1 + Days|Subject, data = sl)
```

## Задание 2

Проверьте получившуюся модель MS1

Сделайте самостоятельно:

- Анализ остатков
- Проверку влияния факторов + подбор оптимальной модели
- Визуализацию предсказаний

## Решение: График остатков от предсказанных значений

```{r purl=FALSE}
MS1_diag <- data.frame(sl, 
                      .resid = resid(MS1, type = "pearson"), 
                      .fitted <- fitted(MS1))
gg_resid_1 <- ggplot(MS1_diag, aes(y = .resid)) +
  guides(colour = guide_legend(ncol = 2))
gg_resid_1 + geom_point(aes(x = .fitted, colour = Subject))
```

\pause

- Есть большие остатки, гетерогенность дисперсий не выражена

## Решение: Графики остатков от ковариат в модели и не в модели

```{r purl=FALSE}
grid.arrange(gg_resid_1 + geom_boxplot(aes(x = factor(Days))),
             gg_resid_1 + geom_boxplot(aes(x = Subject)),
             ncol = 2, widths = c(0.4, 0.6))
```

\pause

- Большие остатки у наблюдений 332 субъекта
- Гетерогенность дисперсий уже не так сильно выражена, как в прошлый раз.

## Решение: Проверка влияния факторов

Тестируем значимость влияния продолжительности бессонницы. Сделаем это при помощи теста отношения правдоподобий.

```{r purl=FALSE}
MS1.ml <- lme(Reaction ~ Days, random = ~1 + Days|Subject, data = sl, 
              method = "ML")
MS2.ml <- update(MS1.ml, . ~ . - Days)
anova(MS1.ml, MS2.ml)
```
\pause

- Время реакции меняется в зависимости от продолжительности бессонницы (L = 24, df = 1, p < 0.01). 

## Решение: Проверка влияния факторов (случайный интерсепт для субъектов)

Почему мы не тестируем значимость самого фактора Subject? 

Потому что этот фактор у нас должен быть в модели по-определению, без обсуждения --- из-за того, что у нас такой дизайн эксперимента.

## Решение: Проверка влияния факторов (случайный угол наклона для субъектов)

Можем проверить, значимы ли изменения угла наклона для разных субъектов.

__Это случайный фактор --- используем REML__

```{r purl=FALSE}
MS1.reml <- lme(Reaction ~ Days, random = ~1 + Days|Subject, data = sl, 
                method = "REML")
MS3.reml <- update(MS1.reml, random = ~1|Subject)
anova(MS1.reml, MS3.reml)
```

\pause

- Скорость изменений зависит от субъекта (L = 43, df = 2, p < 0.01)

## Решение: Представление результатов

Для представления результатов переподбираем модель заново, используя Restricted Maximum Likelihood.

REML оценка параметров более точна (оценка случайных факторов)

```{r purl=FALSE}
MS1_fin <- lme(Reaction ~ Days, random = ~1 + Days|Subject, data = sl,
               method = "REML")
```

Здесь это избыточный шаг, у нас уже есть такая модель --- MS1.reml

## Решение: Уравнение модели

$$Reaction_{ij} = `r round(fixef(MS1_fin), 1)[1]` + `r round(fixef(MS1_fin), 1)[2]` Days_{ij} + b_i + c_{ij} Days_{ij} + \varepsilon_{ij}$$
  
$b_{i} \sim N(0, `r round(as.numeric(VarCorr(MS1_fin)[1, 2]), 1)`^2)$ --- случайный интерсепт для субъекта  
$c_{ij} \sim N(0, `r round(as.numeric(VarCorr(MS1_fin)[2, 2]), 1)`^2)$ --- случайный угол наклона для субъекта  
$\varepsilon_{ij} \sim N(0, `r round(as.numeric(VarCorr(MS1_fin)[3, 2]), 1)`^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни

\vfill
\small

```{r  purl=FALSE}
fixef(MS1_fin)    # Фиксированные эффекты
VarCorr(MS1_fin)  # Случайные эффекты
```

<!-- Внутриклассовая корреляция для моделей со случайным углом наклона --- Goldstein et al. (2002). Не очень полезна, т.к. не может быть интерпретирована просто как доля изменчивости. -->


## Решение: Данные для графика предсказаний фиксированной части модели

```{r purl=FALSE}
# Исходные данные
NewData_MS1 <- ddply(
  sl, .(Subject), summarise,
  Days = seq(min(Days), max(Days), length = 10)
  )

# Предсказанные значения при помощи predict()
# level = 0 - для фиксированных эффектов (т.е. без учета субъекта)
NewData_MS1$fitted <- predict(MS1_fin, NewData_MS1, level = 0)

# Предсказанные значения при помощи матриц
X <- model.matrix(~ Days, data = NewData_MS1)
betas = fixef(MS1_fin)
NewData_MS1$fit <- X %*% betas

# Cтандартные ошибки и дов. интервалы
NewData_MS1$se <- sqrt( diag(X %*% vcov(MS1_fin) %*% t(X)) )
NewData_MS1$lwr <- NewData_MS1$fit - 1.98 * NewData_MS1$se
NewData_MS1$upr <- NewData_MS1$fit + 1.98 * NewData_MS1$se
```

## Решение: График предсказаний фиксированной части модели

```{r purl=FALSE}
ggplot(data = NewData_MS1, aes(x = Days, y = fitted)) +
  geom_ribbon(alpha = 0.35, aes(ymin = lwr, ymax = upr)) +
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))
```

## Решение: Данные для графика предсказаний для индивидуальных уровней случайного фактора

Если вам любопытно, куда делась информация о разных субъектах, то вот она...

Можно получить предсказания для каждого субъекта

$\beta_0 + \beta_1 \cdot Days_{ij} + b_i + c_{ij} \cdot Days_{ij}$

```{r purl=FALSE}
NewData_MS1$fit_subj <- predict(MS1_fin, NewData_MS1, level = 1)
# или то же самое при помощи матриц
# случайные эффекты для каждого субъекта
# это датафрейм с двумя столбцами
rand <- ranef(MS1_fin)
# "разворачиваем" для каждой строки данных
all_rand <- rand[as.numeric(NewData_MS1$Subject), ]
# прибавляем случайные эффекты к предсказаниям фикс. части
NewData_MS1$fit_subj <- (betas[1] + all_rand[, 1]) + (betas[2] + all_rand[, 2]) * NewData_MS1$Days
```


## Решение: График предсказаний для индивидуальных уровней случайного фактора

```{r purl=FALSE}
ggplot(NewData_MS1, aes(x = Days, y = fit_subj, group = Subject)) +
  geom_ribbon(alpha = 0.5, aes(fill = Subject, ymin = fit_subj - 1.98*se,
                  ymax = fit_subj + 1.98*se)) +
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction)) +
  guides(fill = guide_legend(ncol = 2))
```


## Take-home messages

- Смешанные модели могут включать случайные и фиксированные факторы.
    - Градации фиксированных факторов заранее определены, а выводы можно экстраполировать только на такие уровни, которые были задействованы в анализе. Тестируется гипотеза о равенстве средних в группах.
    - Градации случайных факторов --- выборка из возможных уровней, а выводы можно экстраполировать на другие уровни. Тестируется гипотеза о дисперсии между группами.
- Коэффициент внутриклассовой корреляции оценивает, насколько коррелируют друг с другом наблюдения из одной и той же группы случайного фактора.
- Случайные факторы могут описывать вариацию как интерсептов, так и коэффициентов угла наклона.
- Модели со смешанными эффектами позволяют получить предсказания как общем уровне, так и на уровне отдельных субъектов.

## Дополнительные ресурсы

- Crawley, M.J. (2007). The R Book (Wiley).
- Zuur, A. F., Hilbe, J., & Ieno, E. N. (2013). A Beginner's Guide to GLM and GLMM with R: A Frequentist and Bayesian Perspective for Ecologists. Highland Statistics.
- Zuur, A.F., Ieno, E.N., Walker, N., Saveliev, A.A., and Smith, G.M. (2009). Mixed Effects Models and Extensions in Ecology With R (Springer).


